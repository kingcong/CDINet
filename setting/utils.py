def clip_gradient(optimizer, grad_clip):
    """
        Clips gradients computed during backpropagation to avoid explosion of gradients.
        :param optimizer: optimizer with the gradients to be clipped
        :param grad_clip: clip value
    """
    for group in optimizer.param_groups:
        for param in group['params']:
            if param.grad is not None:
                param.grad.data.clamp_(-grad_clip, grad_clip)


def adjust_lr(optimizer, init_lr, epoch, decay_rate=0.1, decay_epoch=30):
    decay = decay_rate ** (epoch // decay_epoch)
    for param_group in optimizer.param_groups:
        param_group['lr'] = decay * init_lr
    return param_group['lr']

